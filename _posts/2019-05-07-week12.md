---
title: "Final Project: Transformer Knowledge Distillation"
date: 2019-05-07
---
<h1> Transformer Knowledge Distillation </h1>
 
After learning about the Transformer and it's incredible capacity to work with language, I was I excited to integrate it 
into a tool for people to use. I had thought about using it for question answering, text summarization, or natural language generation,
so I decided to play with BERT on my laptop. My computer was huffing and puffing, and running out of RAM. Given all this trouble
a laptop was having, I couldn't have imagine what would happen if I tried to run BERT on a phone. This gave me inspiration to consider training
smaller transformer language models with similar levels of performance to the big models. 




{: class="table-of-content"}
* TOC
{:toc}


## Introduction: Transformer Success & Limitations 
![Recent Transformers](/assets/images/transformer_gang_white.png)

## Background:  Knowledge Distillation 
## Interpretation: More Training? 
## Approach: Truncated Softmax 




<h2> LSTM TensorFlow Implimentaiton </h2> 
For a sanity check, I utilized TensorFlow's implementation on the same task as my own implementation( Penn Tree Bank). 
Thankfully, I had identical loss functions :)You can see the colab notebook [here](https://colab.research.google.com/drive/1dZFOcHB2TqBcy2mvdkPERffo-V290MMa).
 Here's the visualization: 

![LSTM Premade Graph](/assets/images/LSTM_premade_graph.png)

