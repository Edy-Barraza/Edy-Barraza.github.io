---
title: "Final Project: Transformer Knowledge Distillation"
date: 2019-05-07
---
<h1> Transformer Knowledge Distillation </h1>
 
After learning about the Transformer and it's incredible capacity to work with language, I was I excited to integrate it 
into a tool for people to use. I had thought about using it for question answering, text summarization, or natural language generation,
so I decided to play with BERT on my laptop. My computer was huffing and puffing, and running out of RAM. Given all this trouble
a laptop was having, I couldn't have imagine what would happen if I tried to run BERT on a phone. This gave me inspiration to consider training
smaller transformer language models with similar levels of performance to the big models. 




{: class="table-of-content"}
* TOC
{:toc}


## Introduction: Transformer Success & Limitations 

![Recent Transformers](/assets/images/transformer_gang_white.png)

Recent transformer models have reached unparalleled success in Natural Language Processing. After GPT, BERT, and GPT-2 are released,
they keep pushing the state of the art in question answering, translation, reading comprehension, text summarization, and more! 
As they become more powerful, they also become larger and larger, causing them to take up more RAM and have longer wait time for inference. 
To use a transformer back a resource for someone seeking resources online in the United States, it is imperative that this resource be available on mobile as most
people have access to the internet on their smartphones but not everyone has a laptop. With the newest development of transformers 
Natural Langauge Processing GPT-2 reaching 1.5 billion parameters, it is necessary to reduce the size of these transformers to ensure
mobile support. I will describe the transformer model to present ways we can reduce it's size!

![Transformer Block](/assets/images/transformer_block.png)
We shall look at the transformer at the highest level and then zoom into its components. The transformer is composed of stacking
multiple transformer blocks! Data flowing through the block encounters a multi-headed attention unit, as well as residually connects
to a Layer Normalization Unit. The output of this layer normalization connects directly to a Feed-Forward Network, and residually connects to
the output of the Feed-Forward Network to another Layer Normalization Unit. 

## Background:  Knowledge Distillation 
## Interpretation: More Training? 
## Approach: Truncated Softmax 




<h2> LSTM TensorFlow Implimentaiton </h2> 
For a sanity check, I utilized TensorFlow's implementation on the same task as my own implementation( Penn Tree Bank). 
Thankfully, I had identical loss functions :)You can see the colab notebook [here](https://colab.research.google.com/drive/1dZFOcHB2TqBcy2mvdkPERffo-V290MMa).
 Here's the visualization: 

![LSTM Premade Graph](/assets/images/LSTM_premade_graph.png)

