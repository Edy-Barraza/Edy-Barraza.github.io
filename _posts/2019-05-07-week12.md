---
title: "Final Project: Transformer Knowledge Distillation"
date: 2019-05-07
---
<h1> Transformer Knowledge Distillation </h1>
 
After learning about the Transformer and it's incredible capacity to work with language, I was I excited to integrate it 
into a tool for people to use. I had thought about using it for question answering, text summarization, or natural language generation,
so I decided to play with BERT on my laptop. My computer was huffing and puffing, and running out of RAM. Given all this trouble
a laptop was having, I couldn't have imagine what would happen if I tried to run BERT on a phone. This gave me inspiration to consider training
smaller transformer language models with similar levels of performance to the big models. 




{: class="table-of-content"}
* TOC
{:toc}


## Introduction: Transformer Success & Limitations 

![Recent Transformers](/assets/images/transformer_gang_white.png)

Recent transformer models have reached unparalleled success in Natural Language Processing. After GPT, BERT, and GPT-2 are released,
they keep pushing the state of the art in question answering, translation, reading comprehension, text summarization, and more! 
As they become more powerful, they also become larger and larger, causing them to take up more RAM and have longer wait time for inference. 
To use a transformer back a resource for someone seeking resources online in the United States, it is imperative that this resource be available on mobile as most
people have access to the internet on their smartphones but not everyone has a laptop. With the newest development of transformers 
Natural Langauge Processing GPT-2 reaching 1.5 billion parameters, it is necessary to reduce the size of these transformers to ensure
mobile support. I will describe the transformer model to present ways we can reduce it's size!

![Transformer Block](/assets/images/transformer_block.png)

We shall look at the transformer at the highest level and then zoom into its components. The transformer is composed of stacking
multiple transformer blocks! Words are first represented as vectors (rank 1 tensor), and thus a sequence of words is represented 
as a list of tensors (rank 2 tensor). This data flowing through a transformer block encounters a multi-headed attention unit, and residually connects
to a Layer Normalization Unit. The output of this layer normalization connects directly to a Feed-Forward Network, and residually connects to
the output of the Feed-Forward Network to another Layer Normalization Unit. 

Layer normalization takes inputs, and each input is normalized along the feature dimension. The Feed-Forward network takes an input and combines 
each of it's features for a new representation. The residual connections help propagate features that could be altered or lost during the 
The expressive power of the Transformer comes from its use of attention, so we will pay special attention to the mutli-headed attention unit.

![Multi-Headed Attention Unit](/assets/images/multiheaded_attention.png)

The multi-headed attention unit first takes the vector representation of words, and then projects these vectors to many lower-dimensional sub-spaces. 
Each of these lower dimensional subspaces can be interpreted as a vector space that emphasizes on certain aspects of human language for a word. So if our multi-headed
attention unit has 8 heads, it's possible that one of these heads projects our word vectors into a space that focuses on the sad aspect of words. 

![Attention Concat](/assets/images/attention_concat.png)
![Attention Compute](/assets/images/attention_compute.png)

For each of these subspaces, the Transformer computes attention on the projected vectors, and concatenates the output of these attention 
computations so we are in the same dimensional vector space as we started off in. The com



## Background:  Knowledge Distillation 
## Interpretation: More Training? 
## Approach: Truncated Softmax 





