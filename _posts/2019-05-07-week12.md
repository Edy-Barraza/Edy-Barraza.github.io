---
title: "Final Project: Transformer Knowledge Distillation"
date: 2019-05-07
---
<h1> Transformer Knowledge Distillation </h1>
 
After learning about the Transformer and it's incredible capacity to work with language, I was I excited to integrate it 
into a tool for people to use. I had thought about using it for question answering, text summarization, or natural language generation,
so I decided to play with BERT on my laptop. My computer was huffing and puffing, and running out of RAM. Given all this trouble
a laptop was having, I couldn't have imagine what would happen if I tried to run BERT on a phone. This gave me inspiration to consider training
smaller transformer language models with similar levels of performance to the big models. 




{: class="table-of-content"}
* TOC
{:toc}


## Introduction: Transformer Success & Limitations 

![Recent Transformers](/assets/images/transformer_gang_white.png)

Recent transformer models have reached unparalleled success in Natural Language Processing. After GPT, BERT, and GPT-2 are released,
they keep pushing the state of the art in question answering, translation, reading comprehension, text summarization, and more! 
As they become more powerful, they also become larger and larger, causing them to take up more RAM and have longer wait time for inference. 
To use a transformer back a resource for someone seeking resources online in the United States, it is imperative that this resource be available on mobile as most
people have access to the internet on their smartphones but not everyone has a laptop. With the newest development of transformers 
Natural Langauge Processing GPT-2 reaching 1.5 billion parameters, it is necessary to reduce the size of these transformers to ensure
mobile support. I will describe the transformer model to present ways we can reduce it's size!

![Transformer Block](/assets/images/transformer_block.png)

We shall look at the transformer at the highest level and then zoom into its components. The transformer is composed of stacking
multiple transformer blocks! Words are first represented as vectors (rank 1 tensor), and thus a sequence of words is represented 
as a list of tensors (rank 2 tensor). This data flowing through a transformer block encounters a multi-headed attention unit, and residually connects
to a Layer Normalization Unit. The output of this layer normalization connects directly to a Feed-Forward Network, and residually connects to
the output of the Feed-Forward Network to another Layer Normalization Unit. 

Layer normalization takes inputs, and each input is normalized along the feature dimension. The Feed-Forward network takes an input and combines 
each of it's features for a new representation. The residual connections help propagate features that could be altered or lost during the 
The expressive power of the Transformer comes from its use of attention, so we will pay special attention to the mutli-headed attention unit.

![Multi-Headed Attention Unit](/assets/images/multiheaded_attention.png)

The multi-headed attention unit first takes the vector representation of words, and then projects these vectors to many lower-dimensional sub-spaces. 
Each of these lower dimensional subspaces can be interpreted as a vector space that emphasizes on certain aspects of human language for a word. So if our multi-headed
attention unit has 8 heads, it's possible that one of these heads projects our word vectors into a space that focuses on the sad aspect of words. 

For each of these subspaces, the Transformer computes attention on the projected vectors, and concatenates the output of these attention 
computations so we are in the same dimensional vector space as we started off in. The attention computation itself gives us the inductive 
bias that makes transformers so much more successful than Recurrent Neural Networks. This inductive bias is that each word in a sequence pays 
a certain amount of attention to the other words in the sequence, paying more attention to the other words that are relevant while paying 
little to no attention to the irrelevant words. This is structured into the scaled dot-product attention. Scaled dot-product attention is computed as such:

The sequences of word vectors (rank 2 tensors) that were projected to lower dimensional subspaces are dotted together, they are scaled by the 
dimension of the vector, and then the softmax across the sequence is computed. This softmax vector is dotted with projected word vectors. This assigns a weight to each word in the sequence, the amount
of attention to pay to that word. 

![Attention Concat](/assets/images/attention_concat.png)
![Attention Compute](/assets/images/attention_compute.png)

Thus, in order to have smaller transformer model, we have the option of reducing the number of transformer blocks, we can reduce the 
original dimension of the word vectors, we can reduce the projected word vector dimension, or we can reduce the number of heads 
in the multi-headed attention unit. 


## Background:  Knowledge Distillation 

## Interpretation: More Training? 
## Approach: Truncated Softmax 





