---
title: "Final Project: Transformer Knowledge Distillation"
date: 2019-05-07
---
## Week 12
 
After learning about the Transformer and it's incredible capacity to work with language, I was I excited to integrate it 
into a tool for people to use. I had thought about using it for question answering, text summarization, or natural language generation,
so I decided to play with BERT on my laptop. My computer was huffing and puffing, and running out of RAM. Given all this trouble
a laptop was having, I couldn't have imagine what would happen if I tried to run BERT on a phone. This gave me inspiration to consider training
smaller transformer language models with similar levels of performance to the big models. 




{: class="table-of-content"}
* TOC
{:toc}


<h2> Introduction: Transformer Success & Limitations </h2>


<h2>Background:  Knowledge Distillation </h2>
<h2>Interpretation: More Training? </h2>
<h2>Approach: Truncated Softmax </h2>




<h2> LSTM TensorFlow Implimentaiton </h2> 
For a sanity check, I utilized TensorFlow's implementation on the same task as my own implementation( Penn Tree Bank). 
Thankfully, I had identical loss functions :)You can see the colab notebook [here](https://colab.research.google.com/drive/1dZFOcHB2TqBcy2mvdkPERffo-V290MMa).
 Here's the visualization: 

![LSTM Premade Graph](/assets/images/LSTM_premade_graph.png)

