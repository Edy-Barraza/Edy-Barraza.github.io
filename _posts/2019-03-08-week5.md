---
title: "Week 5: Fun Implementations"
date: 2019-03-08
---
## Week 5
 
After reading about wonderful neural net architectures for working with sequences of texts, I decided to try some implementations, 
complementing my theoretical understanding with a practical understanding. I decide to give TensorFlow a try
since I had already had experience with the lovely PyTorch. Let me start you off with my my RNN: 


<h2> RNN </h2>
You can find my colab notebook for this implementation [here!](https://colab.research.google.com/drive/1cYqJqlm54Hu3KiHB94WAnfwOrwyVKVma).
Out of curiosity, I wanted to see the TensorBoard visualization, so let me share 2-unit wide visualization with you! 

![Vanilla RNN Graph](/assets/images/rnn_graph.png)

<h2> LSTM </h2>
[Here](https://colab.research.google.com/drive/1mL3arSzK8yU74hy81lKivHeDvya6z0fh) is my google colab notebook for this implementaiton.
I also visualized a 2-unit wide version of my implementation out of curiosity. Take a look! 

![LSTM Graph1](/assets/images/LSTM_class_maingraph.png)

![LSTM Graph2](/assets/images/LSTM_class_auxilary_graph.png)



<h2> LSTM TensorFlow Implimentaiton </h2> 
For a sanity check, I utilized TensorFlow's implementation on the same task as my own implementation( Penn Tree Bank). 
Thankfully, I had identical loss functions :)You can see the colab notebook [here](https://colab.research.google.com/drive/1dZFOcHB2TqBcy2mvdkPERffo-V290MMa).
 Here's the visualization: 

![LSTM Premade Graph](/assets/images/LSTM_premade_graph.png)

