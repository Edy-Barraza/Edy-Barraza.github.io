---
title: "Week 2: Getting A Groove"
date: 2019-02-15
---
## Week 2 
After kicking this week off, it took some time but I feel that I've got a groove I can get with to get things done! 
I was exposed to a lot of  new knowledge this week, so let me share with you my friends, some of the highlights  of what I enjoyed learning! 
I will organize this for you with the following categories: 
<ul>
    <li> Natural Language Processing: Word Embeddings  </li>
    <li> Reinforcement Learning: Multi-Armbed Bandit Problems </li>
    <li> Unsupervised Learning: AutoEncoders, K-Means Clustering </li>
</ul>
Let's get started! 

<h3> Natural Language Processing: Word Embeddings</h3>
My focus for NLP this week was learn vector representations of words. The initial approach was creating count based
matrices. The Word-Document matrix was created by looping through all available documents to create a matrix $$X$$, 
where element $$X_{i,j}$$ tells us if word $$i$$ appears in document $$j$$. This approach is not ideal because of how 
sparse the matrix is, even if it makes use of global statistics of the corpus.

The next attempt is is a Word-Based Co-Occurence Matrix. This matrix $$X_{i,j}$$ is created by looping through the corpus 
and counting how many times each word $$j$$ appears in the context of word $$i$$. This is another sparse matrix, and 
is thus still not ideal. This idea puts us in the line of reasoning of using the context of words. We are interested
in doing so because of the distributional hypothesis.

The distributional hypothesis is the idea that the a word's meaning is given by the words that frequently appear close
by to it. This gave rise to Word2Vec

Word2Vec is composed of two approaches to creating word embeddings, followed by two training methods. Word embeddings can
be generated using the Continuous Bag Of Words (CBOW) model, which aims to the predict a center word given some surrounding
 context, or the Skip-Gram Model, which aims to predict the distribution of context word given a center word. These embeddings can
 be trained using Negative Sampling, which defines an objective by sampling negative samples negative examples from a
 false corpus. These embeddings can also be trained using Hierarchical Soft-Max, which defines an objective using a 
 tree structure to compute probabilities for all words in our vocabulary. 
 
These embeddings had strong performance, but folks wanted to make an improvement by taking into account the distributional 
hypothesis AND utilizing global corpus statistics. This gave rise to GloVe: Global Vectors for Word Representations.
GloVe considers global corpus statistics and the context of words with a co-occurence matrix and a least squares objective.
  

<h3> Reinforcement Learning: Multi-Armbed Bandit Problems</h3> 
We can define elements of reinforcement learning as 
<ul>
    <li> Policy:
        <ul>
            <li>A policy is a mapping from perceived  states of the environemnt to the actions meant to be taken in those states </li>
        </ul>
    </li>
    <li> Reward Signal
        <ul>
            <li> During each time step, the environment sends the agent some # whih is the reward signal</li>
            <li> The agent seeks to maximize the reward it receives in the long run</li>
        </ul>
    </li>
    <li> Value Function
        <ul>
            <li>While the reward specifies what's good in the intermediate sense, the value function specifies what's good in the long run </li>
            <li>Value = amount of reqard the agent can expect to accumulate over the future starting from it's current state</li>
            <li>Allows us to delay gratification and go to a low reward state if we believe we will get more reward in the long run</li>
        </ul>
    </li>
    <li> Optional: Model Of The Environment
        <ul>
            <li> Something that mimics the environment</li>
            <li>Allows us to make inferences about how the environment might behave</li>
            <li>Model might help predict the next state or reward</li>
            <li>Model-Based Methods: Uses models and planning</li>
            <li>Explicitly trail and error learners </li>
        </ul>
    </li>
</ul>
I started my reinforcement learning studies by learning about multi-armed bandits, which is a particular problem 
where we only have one state, and have $$k$$ possible actions that have some expected reward. The action selected at 
some time $$t$$ is $$A_t$$ and returns some reward $$R_t$$. 
The value of action $$a$$ is $$q_{*}(a) = \mathbb{E}[R_t | A_t=a]$$.

We don't really know $$q_{*}(a) $$, but by the law of large numbers, with enough samples we can get a pretty good 
estimate $$Q_t(a)$$. Our principle problem in this setting is balancing exploiting methods we estimate to be the best strategy 
with exploring methods we wish to try out and see if they are successful. 

<h3> Unsupervised Learning: AutoEncoders, K-Means Clustering </h3> 
Autoencoders are nueral networks that seek to seek to learn the funciton $$ h_{w,b}(x) \approx x$$. This sounds trivial
as an identity matrix, but it's not since the number of hidden units in the network is less than the output of the network.
Thus, we have essentially assigned the network a compression task, where it has to find correlations between the inputs. 
We can also have a larger number of hidden units than input and output units, but enforce a sparsity constraint on the hidden
units. This means we want most of the hidden units to be inactive (output close to zero) most of the time. This 
will allow the network to learn correlations between the input of the data. 

K-Means Clustering means to cluster points around some centroid points. The number of centroids you define is arbitray.
Here's the procedure
<ol>
    <li>Initialize cluster centroids randomly: \boldsymbol{\mu_1}...\boldsymbol{\mu_k} \in \mathbb{R}^n  </li>
    <li> </li>
    <li> </li>
    <li> </li>
    <li> </li>
    <li> </li>
</ol>


