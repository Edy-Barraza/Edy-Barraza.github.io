---
title: "Week 2: Getting A Groove"
date: 2019-02-08
---
## Week 2 
After kicking this week off, it took some time but I feel that I've got a groove I can get with to get things done! 
I was exposed to a lot of  new knowledge this week, so let me share with you my friends, some of the highlights  of what I enjoyed learning! 
I will organize this for you with the following categories: 
<ul>
    <li> Natural Language Processing: Word Embeddings  </li>
    <li> Reinforcement Learning: Multi-Armbed Bandit Problems </li>
    <li> Unsupervised Learning: AutoEncoders, K-Means Clustering </li>
</ul>
Let's get started! 

<h3> Natural Language Processing: Word Embeddings</h3>
My focus for NLP this week was learn vector representations of words. The initial approach was creating count based
matrices. The Word-Document matrix was created by looping through all available documents to create a matrix $$X$$, 
where element $$X_{i,j}$$ tells us if word $$i$$ appears in document $$j$$. This approach is not ideal because of how 
sparse the matrix is, even if it makes use of global statistics of the corpus.

The next attempt is is a Word-Based Co-Occurence Matrix. This matrix $$X_{i,j}$$ is created by looping through the corpus 
and counting how many times each word $$j$$ appears in the context of word $$i$$. This is another sparse matrix, and 
is thus still not ideal. This idea puts us in the line of reasoning of using the context of words. We are interested
in doing so because of the distributional hypothesis.

The distributional hypothesis is the idea that the a word's meaning is given by the words that frequently appear close
by to it. This gave rise to Word2Vec

Word2Vec is composed of two approaches to creating word embeddings, followed by two training methods. Word embeddings can
be generated using the Continuous Bag Of Words (CBOW) model, which aims to the predict a center word given some surrounding
 context, or the Skip-Gram Model, which aims to predict the distribution of context word given a center word. These embeddings can
 be trained using Negative Sampling, which defines an objective by sampling negative samples negative examples from a
 false corpus. These embeddings can also be trained using Hierarchical Soft-Max, which defines an objective using a 
 tree structure to compute probabilities for all words in our vocabulary. 
 
These embeddings had strong performance, but folks wanted to make an improvement by taking into account the distributional 
hypothesis AND utilizing global corpus statistics. This gave rise to GloVe: Global Vectors for Word Representations.
GloVe considers global corpus statistics and the context of words with a co-occurence matrix and a least squares objective.

Our co-occurence matrix $$X$$ is again defined such that at  $$X_{i,j}$$, we have the number of times word $$j$$ appears
in the context of word $$i$$. 

We can arrive at our least squares objective by starting off with a Word2Vec Objective. If we say $$Q_{i,j}$$ is the 
soft-max probability of word $$j$$ appearing in the context of word $$i$$, $$Q_{i,j} = \frac{e^{\textbf{u_{j}^{T} v_{i}}}}{\sum_w^{W}e^{\textbf{u_w^T v_i}}}$$
that $$X_i$$ is the number of times any word $$k$$ appears in the context of word $$i$$. 
$$P_{i,j} = P(w_j| w_i) = \frac{X_{i,j}}{X_i}$$, the probability of word $$j$$ appearing in the context of word &&i$$ 
Than we can start with the objective of the 

 
 

<h3> Reinforcement Learning: Multi-Armbed Bandit Problems</h3> 

<h3> Unsupervised Learning: AutoEncoders, K-Means Clustering </h3> 