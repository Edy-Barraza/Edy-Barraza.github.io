---
title: "Week 2: Getting A Groove"
date: 2019-02-08
---
## Week 2 
After kicking this week off, it took some time but I feel that I've got a groove I can get with to get things done! 
I was exposed to a lot of  new knowledge this week, so let me share with you my friends, some of the highlights  of what I enjoyed learning! 
I will organize this for you with the following categories: 
<ul>
    <li> Natural Language Processing: Word Embeddings  </li>
    <li> Reinforcement Learning: Multi-Armbed Bandit Problems </li>
    <li> Unsupervised Learning: AutoEncoders, K-Means Clustering </li>
</ul>
Let's get started! 

<h3> Natural Language Processing: Word Embeddings</h3>
My focus for NLP this week was learn vector representations of words. The initial approach was creating count based
matrices. The Word-Document matrix was created by looping through all available documents to create a matrix $$X$$, 
where element $$X_{i,j}$$ tells us if word $$i$$ appears in document $$j$$. This approach is not ideal because of how 
sparse the matrix is, even if it makes use of global statistics of the corpus.

The next attempt is is a Word-Based Co-Occurence Matrix. This matrix $$X_{i,j}$$ is created by looping through the corpus 
and counting how many times each word $$j$$ appears in the context of word $$i$$. This is another sparse matrix, and 
is thus still not ideal. This idea puts us in the line of reasoning of using the context of words. We are interested
in doing so because of the distributional hypothesis.

The distributional hypothesis is the idea that the a word's meaning is given by the words that frequently appear close
by to it. This gave rise to Word2Vec

<h3> Reinforcement Learning: Multi-Armbed Bandit Problems</h3> 

<h3> Unsupervised Learning:AutoEncoders, K-Means Clustering </h3> 