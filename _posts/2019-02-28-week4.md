---
title: "Week 4: Fun Architectures"
date: 2019-02-28
---
## Week 4
 
I'll be sharing some wonderful neural network architectures for working with sequences of text! Here they are:
<ul>
    <li> Vanilla Recurrent Neural Net (RNN) </li>
    <li> Gated Recurrent United (GRU) </li>
    <li> Long Term Short Memory (LSTM)</li>
</ul>
Note that GRU's and LSTM's are also RNN's. An RNN is a neural network architecture where the connections between
nodes form a time-dependent sequence. Moreover, an RNN utilizes an internal memory for processing sequences in order
to recognize patterns in the data. GRU's and LSTM's are RNN's with more complicated architectures encoding inductive 
biases, allowing them capture more sophisticated patterns in data. 
 unlike
traditional feed-forward networks, 
<h2> Vanilla RNN </h2> 
\begin{equation}
h_t = \sigma(W_h h_{t-1} + W_x x_t)
\end{equation}

\begin{equation}
\hat{y} = softmax(W_s h_t)
\end{equation}

<h2> Gated Recurrent Unit (GRU) </h2> 
\begin{equation}
z_t = \sigma(W_z x_t + U_z h_{t-1})
\end{equation}

\begin{equation}
r_t = \sigma(W_r x_t + U_r h_{t-1})
\end{equation}

\begin{equation}
\tilde{h}_t = \tanh(r_t \odot U h_{t-1} + W x_t)
\end{equation}

\begin{equation}
h_t = (1-z_t)\odot \tilde{h}_t + z_t \odot h_{t-1}
\end{equation}
<h2> Long Term Short Memory (LSTM) </h2> 



\begin{equation}

\end{equation}




