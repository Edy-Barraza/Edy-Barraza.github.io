---
title: "Week 4: Fun Architectures"
date: 2019-02-28
---
## Week 4
 
I'll be sharing some wonderful neural network architectures for working with sequences of text! Here they are:
<ul>
    <li> Vanilla Recurrent Neural Net (RNN) </li>
    <li> Gated Recurrent United (GRU) </li>
    <li> Long Term Short Memory (LSTM)</li>
</ul>
Note that GRU's and LSTM's are also RNN's. An RNN is a neural network architecture where the connections between
nodes form a time-dependent sequence. Moreover, an RNN utilizes an internal memory for processing sequences in order
to recognize patterns in the data. GRU's and LSTM's are RNN's with more complicated architectures encoding inductive 
biases, allowing them capture more sophisticated patterns in data, unlike traditional feed-forward networks.  
<h2> Vanilla RNN </h2> 
![Vanilla RNN](/assets/images/vanilla_rnn.png)

\begin{equation} \tag{Hidden  State}
h_t = \sigma(W_h h_{t-1} + W_x x_t)
\end{equation}

\begin{equation} \tag{Prediction}
\hat{y} = softmax(W_s h_t)
\end{equation}

Suppose we wish to to give an RNN some text, in this case the lyrics to touching song, so that it can predict the next sequence 
of words, in this case so the RNN can keep giving us heartfelt lyrics. Using a vanilla RNN we would start off with a sequence of words 
represented as a sequence of vectors.  Let's say we have the sentence: "I love you for so many reasons, which means I love you 
for all seasons" (from The Fuzz - I Love You For All Seasons). We would represent this as the sequence  $$X = {x_1,x_2,...x_n}$$,
 where $$x_1$$ is a word vector representing "I", $$x_2$$ is a word vector representing  "love", $$x_t$$ is the word at the $$t$$'th time step.   

We would give the first unit of the the RNN the word vector $$x_1$$, and some arbitrary initial past hidden state $$h_0$$ (usually just the zero vector).
and it would create $$h_1$$, a hidden representation the our current word sequence "I". We could then use that hidden representation 
to predict what the next word in our sequence would be, $$\hat{y_1} $$. We would pass $$h_1$$ to the second unit, and it would 
use the next word in the sequence $$x_2$$("love"), to produce $$h_2$$, which is the hidden representation of our current word
sequence "I love". We could then use $$h_2$$ to predict the next word in our sequence $$\hat{y_2} $$. At any given time step 
$$t$$, our RNN will take the representation of our past sequence $$h_{t-1}, the current word $$x_t$$, and produce a hidden representation of our current 
sequence $$h_t$$, and can give us a prediction for the next word in our sequence $$ \hat{y_t}$$. 

Since we process data in a sequential manner, representing our previous sequence as a hidden state vector $$h_{t-1}$$,
 the inductive bias encoded in this neural network architecture is that we can use what we have seen in the past determine 
 what we might see in the future. This is further encoded in our matrices $$W_h$$ and $$W_x$$. Since we multiply $$W_h$$ with our past
 hidden state $$h_{t-1}$$, $$W_h$$ encodes what features of the past are important to determine the future. Since we multiply $$W_x$$ with our
 current word $$x_t$$, $$W_x$$ encodes what features of the present are important to determine the future. 
 
 This model has the right inductive biases for dealing with sequences and is thus successful, but it has very few parameters and thus has 
 less flexibility 
<h2> Gated Recurrent Unit (GRU) </h2> 

![GRU](/assets/images/gru.png)

\begin{equation} \tag{Update Gate}
z_t = \sigma(W_z x_t + U_z h_{t-1})
\end{equation}

\begin{equation} \tag{Reset Gate}
r_t = \sigma(W_r x_t + U_r h_{t-1})
\end{equation}

\begin{equation} \tag{New Memory }
\tilde{h_t} = \tanh(r_t \odot U h_{t-1} + W x_t)
\end{equation}

\begin{equation} \tag{Hidden State}
h_t = (1-z_t)\odot \tilde{h_t} + z_t \odot h_{t-1}
\end{equation}

<h2> Long Term Short Memory (LSTM) </h2> 

![lstm](/assets/images/lstm.png)

\begin{equation} \tag{ Input Gate}
i_t = \sigma(W_i x_t + U_i h_{t-1})
\end{equation}

\begin{equation} \tag{Forget Gate }
f_t = \sigma(W_f x_t + U_f h_{t-1})
\end{equation}

\begin{equation} \tag{Output Gate }
o_t = \sigma(W_o x_t + U_o h_{t-1})
\end{equation}

\begin{equation} \tag{New Memory }
\tilde{c_t} = \tanh(W_c x_t + U_c h_{t-1})
\end{equation}

\begin{equation} \tag{Final Memory} 
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t}
\end{equation}

\begin{equation} \tag{Hidden State}
h_t = o_t \odot \tanh(c_t)
\end{equation}




